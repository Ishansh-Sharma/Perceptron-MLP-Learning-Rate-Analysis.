# -*- coding: utf-8 -*-
"""perceptron_learning

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IwTqfiOSaaeh6ybWSYHZE7n3vPpv5LZo
"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D
from tensorflow.keras.optimizers import SGD

"""#A SINGLE PERCEPTRON"""

#AND database .
X = np.array([[0,0],[0,1],[1,0],[1,1]])
y = np.array([0,0,0,1])

layer_1 = Dense(1, activation='sigmoid')

model = Sequential([layer_1])

#SGD
def train_with(lr):
    model.compile(optimizer = SGD(learning_rate = lr ), loss = 'binary_crossentropy', metrics = ['accuracy'])
    history = model.fit(X,y, epochs = 100, verbose = 0, batch_size = 1)
    weights , bias = model.layers[0].get_weights()
    return history.history['loss'],history.history['accuracy'],weights.flatten(),bias

learning_rates = [0.01, 0.1, 0.5,2.00]
results = {}
weights_lr = []

for lr in learning_rates:
    model = Sequential([layer_1])
    loss, accuracy , weights , biases = train_with(lr)
    results[lr] = (loss, accuracy)
    weights_lr.append((lr, weights, biases))

# Plot Loss vs Epochs
plt.figure(figsize=(18, 8))

# Loss vs Epochs Plot
plt.subplot(2, 2, 1)
for lr, (loss, _) in results.items():
    plt.plot(loss, label=f'lr={lr}')
plt.title('Loss vs Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Learning Rate vs Weights/Biases Plot
plt.subplot(2, 2, 2)
lrs = [entry[0] for entry in weights_lr]
weights = [entry[1] for entry in weights_lr]
biases = [entry[2] for entry in weights_lr]

plt.plot(lrs, [w[0] for w in weights], marker='o', label='Weight 1')
plt.plot(lrs, biases, marker='x', label='Bias')
plt.title('Learning Rate vs Weights/Biases')
plt.xlabel('Learning Rate')
plt.ylabel('Values')
plt.legend()

# Learning Rate vs Final Accuracy Plot
plt.subplot(2, 2, 3)
final_accuracies = [results[lr][1][-1] for lr in results.keys()]  # Get final accuracy for each lr
plt.bar([str(lr) for lr in lrs], final_accuracies, color='green', label='Final Accuracy')
plt.title('Learning Rate vs Final Accuracy')
plt.xlabel('Learning Rate')
plt.ylabel('Accuracy')
plt.legend()

# Learning Rate vs Final Loss Plot
plt.subplot(2, 2, 4)
final_losses = [results[lr][0][-1] for lr in results.keys()]  # Get final loss for each lr
plt.bar([str(lr) for lr in lrs], final_losses, color='red', label='Final Loss')
plt.title('Learning Rate vs Final Cross-Entropy Loss')
plt.xlabel('Learning Rate')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

# Print weights, biases, final accuracy, and final loss for each learning rate
for lr, w, b in weights_lr:
    final_accuracy = results[lr][1][-1]  # Final accuracy for the learning rate
    final_loss = results[lr][0][-1]      # Final loss for the learning rate
    print(f"Learning Rate: {lr}")
    print(f"  Weights: {w}")
    print(f"  Biases: {b}")
    print(f"  Final Accuracy: {final_accuracy:.4f}")
    print(f"  Final Loss: {final_loss:.4f}\n")

#GD
def train_with(lr):
    model.compile(optimizer = SGD(learning_rate = lr ), loss = 'binary_crossentropy', metrics = ['accuracy'])
    history = model.fit(X,y, epochs = 100, verbose = 0, batch_size = len(X) )
    weights , bias = model.layers[0].get_weights()
    return history.history['loss'],history.history['accuracy'],weights.flatten(),bias

lr_list = [0.01,0.1,0.3,0.5,1.00]
weights_per_lr = []
results = {}

for lr in lr_list:
    model = Sequential([layer_1])
    loss, accuracy , weights , biases = train_with(lr)
    results[lr] = (loss, accuracy)
    weights_per_lr.append((lr, weights, biases))

# Plot Loss vs Epochs
plt.figure(figsize=(18, 8))

# Loss vs Epochs Plot
plt.subplot(2, 2, 1)
for lr, (loss, _) in results.items():
    plt.plot(loss, label=f'lr={lr}')
plt.title('Loss vs Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Learning Rate vs Weights/Biases Plot
plt.subplot(2, 2, 2)
lrs = [entry[0] for entry in weights_per_lr]
weights = [entry[1] for entry in weights_per_lr]
biases = [entry[2] for entry in weights_per_lr]

plt.plot(lrs, [w[0] for w in weights], marker='o', label='Weight 1')
plt.plot(lrs, biases, marker='x', label='Bias')
plt.title('Learning Rate vs Weights/Biases')
plt.xlabel('Learning Rate')
plt.ylabel('Values')
plt.legend()

# Learning Rate vs Final Accuracy Plot
plt.subplot(2, 2, 3)
final_accuracies = [results[lr][1][-1] for lr in results.keys()]  # Get final accuracy for each lr
plt.bar([str(lr) for lr in lrs], final_accuracies, color='green', label='Final Accuracy')
plt.title('Learning Rate vs Final Accuracy')
plt.xlabel('Learning Rate')
plt.ylabel('Accuracy')
plt.legend()

# Learning Rate vs Final Loss Plot
plt.subplot(2, 2, 4)
final_losses = [results[lr][0][-1] for lr in results.keys()]  # Get final loss for each lr
plt.bar([str(lr) for lr in lrs], final_losses, color='red', label='Final Loss')
plt.title('Learning Rate vs Final Cross-Entropy Loss')
plt.xlabel('Learning Rate')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

# Print weights, biases, final accuracy, and final loss for each learning rate
for lr, w, b in weights_per_lr:
    final_accuracy = results[lr][1][-1]  # Final accuracy for the learning rate
    final_loss = results[lr][0][-1]      # Final loss for the learning rate
    print(f"Learning Rate: {lr}")
    print(f"  Weights: {w}")
    print(f"  Biases: {b}")
    print(f"  Final Accuracy: {final_accuracy:.4f}")
    print(f"  Final Loss: {final_loss:.4f}\n")

"""#Multi Layer Perceptron
will use for bigger and complex dataset and application of different activation functions and more optimizers etc.
"""

import sklearn
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import f1_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import log_loss

cancer = load_breast_cancer()

df = pd.DataFrame(cancer.data, columns=cancer.feature_names)
df['target'] = cancer.target

print(df.head())
print(df.describe())

X = df.drop('target', axis=1)
y = df['target']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

layer_11 = Dense(5, activation='relu')
layer_2 = Dense(2, activation='relu')
layer_3 = Dense(1, activation='sigmoid')

model_new = Sequential([layer_11, layer_2, layer_3])

def train_with_parameteres(lr,batch_size):
  model_new.compile(optimizer = SGD(learning_rate = lr ), loss = 'binary_crossentropy', metrics = ['Recall'])
  history = model_new.fit(X_train,y_train, epochs = 100, verbose = 0, batch_size = batch_size)
  evaluation = model_new.evaluate(X_test,y_test)
  weights , bias = model_new.layers[0].get_weights()
  return history.history['loss'] ,evaluation[0],evaluation[1] , weights.flatten(),bias

learning_rate = [0.01,0.1,0.4,0.8,1.0]
weights_per_lr = []
results_new = {}

#SGD
for lr in learning_rate:
    model_new = Sequential([layer_11, layer_2, layer_3])
    loss_per_epoch , final_loss , recall , weights , biases  = train_with_parameteres(lr,1)
    results_new[lr] = (loss_per_epoch, final_loss, recall)
    weights_per_lr.append((lr, weights, biases))

# Plot Loss, Final Loss, Recall, and Weights/Biases
plt.figure(figsize=(16, 8))

# Loss Plot
plt.subplot(2, 2, 1)
for lr, (loss, _, _) in results_new.items():  # Unpacking three elements
    plt.plot(loss, label=f'lr={lr}')
plt.title('Training Loss vs Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Final Loss Plot
plt.subplot(2, 2, 2)
lrs = []
final_losses = []
for lr, (_, final_loss, _) in results_new.items():  # Unpacking three elements
    lrs.append(lr)
    final_losses.append(final_loss)
plt.bar([str(lr) for lr in lrs], final_losses, color='salmon')
plt.title('Final Loss for Different Learning Rates')
plt.xlabel('Learning Rate')
plt.ylabel('Loss')

# Test Recall Plot
plt.subplot(2, 2, 3)
lrs = []
recalls = []
for lr, (_, _, recall) in results_new.items():  # Unpacking three elements
    lrs.append(lr)
    recalls.append(recall)
plt.bar([str(lr) for lr in lrs], recalls, color='skyblue')
plt.title('Test Recall for Different Learning Rates')
plt.xlabel('Learning Rate')
plt.ylabel('Recall')

# Weights and Biases Plot
plt.subplot(2, 2, 4)
weights = []
biases = []
for lr, w, b in weights_per_lr:  # Extract weights and biases
    weights.append(w)
    biases.append(b)

# Plot weights and biases
plt.plot(lrs, [w[0] for w in weights], marker='o', label='Weight 1')
if len(weights[0]) > 1:  # Check if there's more than one weight
    plt.plot(lrs, [w[1] for w in weights], marker='o', label='Weight 2')
plt.plot(lrs, biases, marker='x', label='Bias')
plt.title('Learning Rate vs Weights/Biases')
plt.xlabel('Learning Rate')
plt.ylabel('Values')
plt.legend()

plt.tight_layout()
plt.show()

# Print Weights and Biases
for lr, w, b in weights_per_lr:
    print(f"Learning Rate: {lr}")
    print(f"  Weights: {w}")
    print(f"  Biases: {b}\n")

results_gradient = {}

#Gradient Descent :
for lr in learning_rate:
    model_new = Sequential([layer_11, layer_2, layer_3])
    loss_per_epoch, final_loss, recall , weights , biases = train_with_parameteres(lr, len(X_train))  # Training the model
    results_gradient[lr] = (loss_per_epoch, final_loss, recall)

# Plotting the results
plt.figure(figsize=(15, 6))

# Loss vs Epoch Plot
plt.subplot(1, 3, 1)
for lr, (loss_per_epoch, _, _) in results_gradient.items():
    plt.plot(loss_per_epoch, label=f'lr={lr}')
plt.title('Loss vs Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Final Loss Bar Plot
plt.subplot(1, 3, 2)
lrs = []
final_losses = []
for lr, (_, final_loss, _) in results_gradient.items():
    lrs.append(lr)
    final_losses.append(final_loss)
plt.bar([str(lr) for lr in lrs], final_losses, color='salmon')
plt.title('Final Loss for Different Learning Rates')
plt.xlabel('Learning Rate')
plt.ylabel('Final Loss')

# Test Recall Bar Plot
plt.subplot(1, 3, 3)
recalls = []
for lr, (_, _, recall) in results_gradient.items():
    recalls.append(recall)
plt.bar([str(lr) for lr in lrs], recalls, color='skyblue')
plt.title('Test Recall for Different Learning Rates')
plt.xlabel('Learning Rate')
plt.ylabel('Recall')

plt.tight_layout()
plt.show()

# Print Test Recall for each learning rate
for lr, (_, _, test_recall) in results_gradient.items():
    print(f"Learning Rate: {lr}\n  Test Recall: {test_recall:.4f}\n")

#now using the RMS prop in the sme
def train_rms_prop(lr):
  model_new.compile(optimizer = keras.optimizers.RMSprop(learning_rate = lr ), loss = 'binary_crossentropy', metrics = ['Recall'])
  history = model_new.fit(X_train,y_train, epochs = 100, verbose = 0, batch_size = 1)
  evaluation = model_new.evaluate(X_test,y_test)

  return history.history['loss'],evaluation[0],evaluation[1]

learning_rate_new = [0.01,0.02,0.05,0.8,0.1,0.5]
results_rms = {}

for lr in learning_rate_new:
    model_new = Sequential([layer_11, layer_2, layer_3])
    loss_per_epoch, fianl_loss , recall = train_rms_prop(lr)
    results_rms[lr] = (loss_per_epoch , final_loss , recall)

# Plotting the results
plt.figure(figsize=(15, 6))

# Loss vs Epoch Plot
plt.subplot(1, 3, 1)
for lr, (loss_per_epoch, _, _) in results_rms.items():
    plt.plot(loss_per_epoch, label=f'lr={lr}')
plt.title('Loss vs Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Final Loss Bar Plot
plt.subplot(1, 3, 2)
lrs = []
final_losses = []
for lr, (_, final_loss, _) in results_rms.items():
    lrs.append(lr)
    final_losses.append(final_loss)
plt.bar([str(lr) for lr in lrs], final_losses, color='salmon')
plt.title('Final Loss for Different Learning Rates')
plt.xlabel('Learning Rate')
plt.ylabel('Final Loss')

# Test Recall Bar Plot
plt.subplot(1, 3, 3)
recalls = []
for lr, (_, _, recall) in results_rms.items():
    recalls.append(recall)
plt.bar([str(lr) for lr in lrs], recalls, color='skyblue')
plt.title('Test Recall for Different Learning Rates')
plt.xlabel('Learning Rate')
plt.ylabel('Recall')

plt.tight_layout()
plt.show()

# Print Test Recall for each learning rate
for lr, (_, _, test_recall) in results_gradient.items():
    print(f"Learning Rate: {lr}\n  Test Recall: {test_recall:.4f}\n")

#momentum application

def train_momentum(lr,momentum):
  model_new.compile(optimizer = keras.optimizers.SGD(learning_rate = lr , momentum = momentum), loss = 'binary_crossentropy', metrics = ['Recall'])
  history = model_new.fit(X_train,y_train, epochs = 100, verbose = 0, batch_size = 1)
  evaluation = model_new.evaluate(X_test,y_test)

  return history.history['loss'],evaluation[0],evaluation[1]

learning_rate_new = [0.001,0.008,0.01,0.05,0.1]
results_momentum = {}

for lr in learning_rate_new:
    model_new = Sequential([layer_11, layer_2, layer_3])
    loss_per_epoch, final_loss , recall = train_momentum(lr,0.9)
    results_momentum[lr] = (loss_per_epoch , final_loss , recall)
    print(results_momentum)

# Plotting the results
plt.figure(figsize=(15, 6))

# Loss vs Epoch Plot
plt.subplot(1, 3, 1)
for lr, (loss_per_epoch, _, _) in results_momentum.items():
    plt.plot(loss_per_epoch, label=f'lr={lr}')
plt.title('Loss vs Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Final Loss Bar Plot
plt.subplot(1, 3, 2)
lrs = []
final_losses = []
for lr, (_, final_loss, _) in results_momentum.items():
    lrs.append(lr)
    final_losses.append(final_loss)
plt.bar([str(lr) for lr in lrs], final_losses, color='salmon')
plt.title('Final Loss for Different Learning Rates')
plt.xlabel('Learning Rate')
plt.ylabel('Final Loss')

# Test Recall Bar Plot
plt.subplot(1, 3, 3)
recalls = []
for lr, (_, _, recall) in results_momentum.items():
    recalls.append(recall)
plt.bar([str(lr) for lr in lrs], recalls, color='skyblue')
plt.title('Test Recall for Different Learning Rates')
plt.xlabel('Learning Rate')
plt.ylabel('Recall')

plt.tight_layout()
plt.show()

# Print Test Recall for each learning rate
for lr, (_, _, test_recall) in results_momentum.items():
    print(f"Learning Rate: {lr}\n  Test Recall: {test_recall:.4f}\n")

def train_adagrad(lr):
  model_new.compile(optimizer = keras.optimizers.Adagrad(learning_rate = lr ), loss = 'binary_crossentropy', metrics = ['Recall'])
  history = model_new.fit(X_train,y_train, epochs = 100, verbose = 0, batch_size = 1)
  evaluation = model_new.evaluate(X_test,y_test)

  return history.history['loss'],evaluation[0],evaluation[1]

learning_rate_new = [0.001,0.008,0.01,0.05,0.1]
results_adagrad = {}

for lr in learning_rate_new:
    model_new = Sequential([layer_11, layer_2, layer_3])
    loss_per_epoch, final_loss , recall = train_adagrad(lr)
    results_adagrad[lr] = (loss_per_epoch , final_loss , recall)

# Plotting the results
plt.figure(figsize=(15, 6))

# Loss vs Epoch Plot
plt.subplot(1, 3, 1)
for lr, (loss_per_epoch, _, _) in results_adagrad.items():
    plt.plot(loss_per_epoch, label=f'lr={lr}')
plt.title('Loss vs Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Final Loss Bar Plot
plt.subplot(1, 3, 2)
lrs = []
final_losses = []
for lr, (_, final_loss, _) in results_adagrad.items():
    lrs.append(lr)
    final_losses.append(final_loss)
plt.bar([str(lr) for lr in lrs], final_losses, color='salmon')
plt.title('Final Loss for Different Learning Rates')
plt.xlabel('Learning Rate')
plt.ylabel('Final Loss')

# Test Recall Bar Plot
plt.subplot(1, 3, 3)
recalls = []
for lr, (_, _, recall) in results_adagrad.items():
    recalls.append(recall)
plt.bar([str(lr) for lr in lrs], recalls, color='skyblue')
plt.title('Test Recall for Different Learning Rates')
plt.xlabel('Learning Rate')
plt.ylabel('Recall')

plt.tight_layout()
plt.show()

# Print Test Recall for each learning rate
for lr, (_, _, test_recall) in results_.items():
    print(f"Learning Rate: {lr}\n  Test Recall: {test_recall:.4f}\n")

